{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5: Enhancing RAG with Structured Metadata\n",
    "\n",
    "Effective RAG systems need more than just text matching - they need to understand and filter based on specific attributes. This notebook demonstrates how to use LLMs to generate structured metadata that enables more precise and relevant retrieval.\n",
    "\n",
    "## Why Generate Metadata with LLMs?\n",
    "\n",
    "Consider a typical e-commerce query: \"I'm looking for a black cotton t-shirt under $50\"\n",
    "\n",
    "This simple request contains multiple filtering criteria:\n",
    "- Color (black)\n",
    "- Material (cotton)\n",
    "- Product type (t-shirt)\n",
    "- Price range (< $50)\n",
    "\n",
    "Traditional text search struggles with such queries because it needs:\n",
    "1. Structured data for filtering\n",
    "2. Consistent attribute labeling\n",
    "3. Standardized taxonomies\n",
    "\n",
    "## Benefits of LLM Metadata Generation\n",
    "\n",
    "1. **Consistency**\n",
    "   - Standardized attribute extraction\n",
    "   - Taxonomy compliance\n",
    "   - Quality control at scale\n",
    "\n",
    "2. **Efficiency**\n",
    "   - Faster than manual labeling\n",
    "   - More accurate than rule-based systems\n",
    "   - Cost-effective for large catalogs\n",
    "\n",
    "3. **Flexibility**\n",
    "   - Adapts to new product types\n",
    "   - Handles complex attributes\n",
    "   - Supports taxonomy updates\n",
    "\n",
    "## Our Approach\n",
    "\n",
    "We'll build a metadata generation system in three phases:\n",
    "\n",
    "1. **Taxonomy Definition**\n",
    "   - Define product categories\n",
    "   - Specify valid attributes\n",
    "   - Create validation rules\n",
    "\n",
    "2. **Data Generation**\n",
    "   - Process product images\n",
    "   - Extract structured metadata\n",
    "   - Validate against taxonomy\n",
    "\n",
    "3. **Quality Assurance**\n",
    "   - Enforce consistency\n",
    "   - Validate attributes\n",
    "   - Enable team collaboration\n",
    "\n",
    "## What We'll Create\n",
    "\n",
    "Using the `irow/ClothingControlV2` dataset, we'll:\n",
    "1. Define a robust e-commerce taxonomy\n",
    "2. Generate structured product metadata\n",
    "3. Create a validation pipeline\n",
    "4. Enable efficient filtering\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic understanding of e-commerce data\n",
    "- Familiarity with product taxonomies\n",
    "- Python environment with required libraries\n",
    "\n",
    "Let's dive in and see how LLMs can help create better structured data for RAG systems!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating our Dataset\n",
    "\n",
    "In this portion, we'll generate a dataset that mimics a e-commerce company's product catalog. In order to do so, we'll be extracting out item data from images using `gpt-4o` and then using a taxonomy to classify the items.\n",
    "\n",
    "## Loading in our Taxonomy\n",
    "\n",
    "E-Commerce companies use what's called a taxonomy to classify their products. In our case, we've chosen the following fields\n",
    "\n",
    "- `category` : This is a high level category such as Men's, Women's, Unisex, etc.\n",
    "- `subcategory` : This is a more specific category such as T-Shirts, Blouses that are under a specific category\n",
    "- `types` : These are more specific product types such as Crew Neck T-Shirt, V-Neck T-Shirt, etc.\n",
    "- `attributes` These are attributes that are specific to the items that have that specific category, subcategory and type combination.\n",
    "- `common_attributes` These are attributes that are common to all items in our database such as sizes and colors in stock\n",
    "\n",
    "We want to define a taxonomy ahead of time for three main reasons\n",
    "\n",
    "1. **Consistency** : By having a consistent taxonomy, we can ensure that we only generate data on items that fall within our taxonomy. \n",
    "2. **Filtering** : It makes it easy for us to map user queries to a set of known metadata fields which we can use to filter our retrieved items down the line.\n",
    "3. **Non-Technical Help**: By using a human-readable format like yaml, we can ask members of our team that aren't technical to help define the proper taxonomy. You can implement this too using a no-sql database to store raw taxonomies or configs but we've chosen to keep it simply for now.\n",
    "\n",
    "Let's now read in our taxonomy and see how we can enforce these fields. We'll define some simple Pydantic models to make it easy for us to work with the yaml data\n",
    "\n",
    "We've defined a `progress_taxonomy_file` function to help us process the yaml file and convert it into a dictionary. We use `pydantic` and `instructor` to help make sure that our LLM generated metadata conforms to our taxonomy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['taxonomy_map', 'occasions', 'materials', 'common_attributes', 'taxonomy'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from helpers import process_taxonomy_file\n",
    "\n",
    "taxonomy_data = process_taxonomy_file(\"taxonomy.yml\")\n",
    "\n",
    "taxonomy_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Size', 'Color', 'Material', 'Pattern', 'Occasion'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxonomy_data[\"common_attributes\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import model_validator, ValidationInfo, BaseModel\n",
    "\n",
    "\n",
    "class ItemAttribute(BaseModel):\n",
    "    name: str\n",
    "    value: str\n",
    "\n",
    "\n",
    "class ItemMetadata(BaseModel):\n",
    "    title: str\n",
    "    brand: str\n",
    "    description:str\n",
    "    category: str\n",
    "    subcategory: str\n",
    "    product_type: str\n",
    "    attributes: list[ItemAttribute]\n",
    "    material: str\n",
    "    pattern: str\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate_material_and_pattern(self, info: ValidationInfo):\n",
    "        context = info.context\n",
    "        if not context or not context[\"taxonomy_data\"]:\n",
    "            raise ValueError(\"Taxonomy data is required for validation\")\n",
    "\n",
    "        if self.pattern not in context[\"taxonomy_data\"][\"common_attributes\"][\"Pattern\"]:\n",
    "            raise ValueError(\n",
    "                f\"Pattern {self.pattern} is not a valid pattern. Valid patterns are {context['taxonomy_data']['common_attributes']['Pattern']}\"\n",
    "            )\n",
    "\n",
    "        if self.material not in context[\"taxonomy_data\"][\"common_attributes\"][\"Material\"]:\n",
    "            raise ValueError(\n",
    "                f\"Material {self.material} is not a valid material. Valid materials are {context['taxonomy_data']['common_attributes']['Material']}\"\n",
    "            )\n",
    "\n",
    "        return self\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate_category_and_attributes(self, info: ValidationInfo):\n",
    "        context = info.context\n",
    "        if not context or not context[\"taxonomy_data\"]:\n",
    "            raise ValueError(\"Taxonomy data is required for validation\")\n",
    "\n",
    "        taxonomy_map = context[\"taxonomy_data\"][\"taxonomy_map\"]\n",
    "\n",
    "        # 1. Validate category\n",
    "        if self.category not in taxonomy_map:\n",
    "            raise ValueError(\n",
    "                f\"Category {self.category} is not valid. Valid categories are {list(taxonomy_map.keys())}\"\n",
    "            )\n",
    "\n",
    "        # 2. Validate subcategory\n",
    "        if self.subcategory not in taxonomy_map[self.category]:\n",
    "            raise ValueError(\n",
    "                f\"Subcategory {self.subcategory} does not exist under category {self.category}\"\n",
    "            )\n",
    "\n",
    "        subcategory_data = taxonomy_map[self.category][self.subcategory]\n",
    "\n",
    "        # 3. Validate product type\n",
    "        if self.product_type not in subcategory_data[\"product_type\"]:\n",
    "            raise ValueError(\n",
    "                f\"Product type {self.product_type} is not valid for subcategory {self.subcategory}. Valid types are {subcategory_data['product_type']}\"\n",
    "            )\n",
    "\n",
    "        # 4. Validate attributes\n",
    "        for attr in self.attributes:\n",
    "            if attr.name not in subcategory_data[\"attributes\"]:\n",
    "                raise ValueError(\n",
    "                    f\"Attribute {attr.name} is not valid for subcategory {self.subcategory}. Valid attributes are {list(subcategory_data['attributes'].keys())}\"\n",
    "                )\n",
    "\n",
    "            if attr.value not in subcategory_data[\"attributes\"][attr.name]:\n",
    "                raise ValueError(\n",
    "                    f\"Value {attr.value} is not valid for attribute {attr.name}. Valid values are {subcategory_data['attributes'][attr.name]}\"\n",
    "                )\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Item Data from Images\n",
    "\n",
    "The `irow/ClothingControlV2` dataset contains images of clothing items that are generated using a control net. It doesn't have any product data and so we'll extract out the item data from the images. \n",
    "\n",
    "We use `instructor` here to help us extract the item data from the images. Note here that we're rendering the entire yml file as context. We want to do so for two reasons\n",
    "\n",
    "1. Firstly, providing all of the possible choices allows the model more flexibility in deciding what the right metadata fields are\n",
    "2. Secondly, if we have a large taxonomy, we can leverage techniques like prompt caching to save on costs. By ensuring that the initial portion of the prompt is the same, we can leverage caching to speed up the extraction process.\n",
    "\n",
    "We'll be using `gpt-4o` here for the extraction since it supports multimodal inputs ( in this case images )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = [item for item in load_dataset(\"irow/ClothingControlV2\",streaming=True)[\"train\"].take(2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ItemMetadata(title='Lace Detail Sleeveless Top', brand='H&M', description='Elevate your wardrobe with this elegant sleeveless top featuring intricate lace detailing at the neckline. Perfect for a chic daytime look or a night out, this versatile piece combines comfort and style effortlessly.', category='Women', subcategory='Tops', product_type='Tank Tops', attributes=[ItemAttribute(name='Sleeve Length', value='Sleeveless'), ItemAttribute(name='Neckline', value='Crew Neck'), ItemAttribute(name='Fit', value='Regular')], material='Cotton', pattern='Solid')]\n"
     ]
    }
   ],
   "source": [
    "from openai import AsyncOpenAI\n",
    "import instructor\n",
    "import tempfile\n",
    "\n",
    "client = instructor.from_openai(AsyncOpenAI())\n",
    "\n",
    "with open(\"taxonomy.yml\", \"r\") as f:\n",
    "    taxonomy = f.read()\n",
    "\n",
    "with tempfile.NamedTemporaryFile(delete=False, suffix=\".png\") as f:\n",
    "    ds[0][\"image\"].save(f.name)\n",
    "    items = await client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an expert at extracting item data from images. Extract 1-2 items seen in the images based on the taxonomy provided. Here are the categories, subcategories, types and attributes that you can choose from: {{ taxonomy_data['taxonomy_map'] }}\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    \"Here is the image, choose a brand that likely sells these items - choose a real brand that exists in real life and make up a name if that's not possible. Also generate a short description of 1-2 sentences of the item that would be suitable for an e-commerce website\",\n",
    "                    instructor.Image.from_path(f.name),\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "        response_model=list[ItemMetadata],\n",
    "        context={\n",
    "            \"taxonomy_data\": taxonomy_data\n",
    "        },\n",
    "        \n",
    "    )\n",
    "\n",
    "    print(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `gpt-4o` was able to extract out the item data from the image and that the metadata fields conform to the taxonomy that we've defined. We've extracted out a list of items that are in the image and mapped them to a category, subcategory and other metadata fields. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "import instructor\n",
    "import tempfile\n",
    "from asyncio import Semaphore, timeout\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed\n",
    "from tqdm.asyncio import tqdm_asyncio as asyncio\n",
    "\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_fixed(1))\n",
    "async def generate_dataset_label(\n",
    "    dataset_item: dict, client: instructor.AsyncInstructor, sem: Semaphore, taxonomy_data: dict\n",
    "):\n",
    "    async with sem, timeout(30):\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".png\") as f:\n",
    "            dataset_item[\"image\"].save(f.name)\n",
    "            items = await client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are an expert at extracting item data from images. Extract 1-2 items seen in the images based on the taxonomy provided. Here are the categories, subcategories, types and attributes that you can choose from: {{ taxonomy_data['taxonomy_map'] }}\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            \"Here is the image, choose a brand that likely sells these items - choose a real brand that exists in real life and make up a name if that's not possible. Also generate a short description of 1-2 sentences of the item that would be suitable for an e-commerce website\",\n",
    "                            instructor.Image.from_path(f.name),\n",
    "                        ],\n",
    "                    },\n",
    "                ],\n",
    "                response_model=list[ItemMetadata],\n",
    "                context={\n",
    "                    \"taxonomy_data\": taxonomy_data\n",
    "                },\n",
    "                \n",
    "            )\n",
    "\n",
    "            return [\n",
    "                {\"image\": dataset_item[\"image\"], \"metadata\": item} for item in items\n",
    "            ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [02:19<00:00,  1.08it/s]\n"
     ]
    }
   ],
   "source": [
    "import instructor\n",
    "\n",
    "client = instructor.from_openai(AsyncOpenAI())\n",
    "sem = Semaphore(15)\n",
    "n_rows = 150\n",
    "\n",
    "ds = [item for item in load_dataset(\"irow/ClothingControlV2\",streaming=True)[\"train\"].take(n_rows)]\n",
    "results = await asyncio.gather(\n",
    "    *[generate_dataset_label(ds_row, client, sem, taxonomy_data) for ds_row in ds]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can create a dataset, we need to flatten the list of items we've extracted from the images. We'll also flatten the metadata fields so that we can create a dataset with the correct schema. Since our attributes are a nested list of objects, we'll convert them to a json string for convinience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Lace Detail Sleeveless Top',\n",
       " 'brand': 'H&M',\n",
       " 'description': \"Elevate your casual wardrobe with this elegant sleeveless top featuring intricate lace detailing at the neckline. Perfect for both day and night, it's crafted from a soft, breathable fabric for all-day comfort.\",\n",
       " 'category': 'Women',\n",
       " 'subcategory': 'Tops',\n",
       " 'product_type': 'Tank Tops',\n",
       " 'attributes': [{'name': 'Sleeve Length', 'value': 'Sleeveless'},\n",
       "  {'name': 'Neckline', 'value': 'Crew Neck'}],\n",
       " 'material': 'Cotton',\n",
       " 'pattern': 'Solid'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten results list of lists into a single list\n",
    "flattened_results = [item for sublist in results for item in sublist]\n",
    "flattened_results[0][\"metadata\"].model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=768x1024>,\n",
       " 'title': 'Lace Detail Sleeveless Top',\n",
       " 'brand': 'H&M',\n",
       " 'description': \"Elevate your casual wardrobe with this elegant sleeveless top featuring intricate lace detailing at the neckline. Perfect for both day and night, it's crafted from a soft, breathable fabric for all-day comfort.\",\n",
       " 'category': 'Women',\n",
       " 'subcategory': 'Tops',\n",
       " 'product_type': 'Tank Tops',\n",
       " 'attributes': '[{\"name\": \"Sleeve Length\", \"value\": \"Sleeveless\"}, {\"name\": \"Neckline\", \"value\": \"Crew Neck\"}]',\n",
       " 'material': 'Cotton',\n",
       " 'pattern': 'Solid',\n",
       " 'id': 1,\n",
       " 'price': 181.04}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def flatten_item(item: dict, id: int):\n",
    "    flattened_item = {\"image\": item[\"image\"], **item[\"metadata\"].model_dump()}\n",
    "\n",
    "    return {\n",
    "        **flattened_item,\n",
    "        \"id\": id,\n",
    "        \"price\": round(random.uniform(10.0, 400.0),2),\n",
    "\n",
    "        \"attributes\": json.dumps(flattened_item[\"attributes\"]),\n",
    "    }\n",
    "\n",
    "\n",
    "hf_dataset_items = [flatten_item(item, id+1) for id, item in enumerate(flattened_results)]\n",
    "hf_dataset_items[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 191/191 [00:00<00:00, 21290.32 examples/s]/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 317.61ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:03<00:00,  3.01s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/ivanleomk/ecommerce-taxonomy/commit/f404c96bf9e1ec0d3de7026312f5bcd36f18ceef', commit_message='Upload dataset', commit_description='', oid='f404c96bf9e1ec0d3de7026312f5bcd36f18ceef', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/ivanleomk/ecommerce-taxonomy', endpoint='https://huggingface.co', repo_type='dataset', repo_id='ivanleomk/ecommerce-taxonomy'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's create a new HF dataset with the labelled data so that we have it stored\n",
    "# Convert to HuggingFace Dataset format\n",
    "from datasets import Dataset\n",
    "\n",
    "# Create HF dataset\n",
    "dataset = Dataset.from_list(hf_dataset_items)\n",
    "dataset.push_to_hub(\"ivanleomk/ecommerce-taxonomy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we've demonstrated how to enhance RAG systems with structured metadata generated by LLMs. This builds on our query understanding work from Week 4 while adding a new dimension of structured data.\n",
    "\n",
    "Key accomplishments:\n",
    "1. Developed a robust taxonomy for metadata\n",
    "2. Created an LLM-powered metadata generation pipeline\n",
    "3. Implemented validation to ensure metadata quality\n",
    "\n",
    "This work takes our RAG system beyond pure text matching, leveraging the insights from our query classification system in Week 4. In the next notebook, we'll show how to use this metadata to improve retrieval performance.\n",
    "\n",
    "Looking ahead to Week 6, this structured metadata approach will help our tool selection system make more informed decisions. The validation techniques we've developed here will also prove valuable for ensuring reliable tool selection.\n",
    "\n",
    "Remember that metadata schemas should evolve with your application - start simple and add complexity only as needed based on user requirements and performance metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
